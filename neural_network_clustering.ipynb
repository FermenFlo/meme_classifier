{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import uuid\n",
    "import keras\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn import svm\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "from bitstring import BitArray\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a model that can do one thing: Differentiate two images as being the same meme (target variable 1) or different memes (target variable 0). This boils down to a simple binary classification problem. The model works by comparing the XNOR value of two image's perceptual hash. This is possible because similar images have similar [perceptual hashes](https://en.wikipedia.org/wiki/Perceptual_hashing).\n",
    "\n",
    "This model was then leveraged to create an algorithm that would create \"groups\" of memes by iteratively asking the question \"are these two memes the same?\". By doing so, I was somewhat succesful in classifying a database of memes.\n",
    "\n",
    "The pipeline is as follows:\n",
    " \n",
    "* Images are stored in the \"IMAGES\" directory\n",
    "* A perceptual hash of each image is stored in memory.\n",
    "* A trainging set is built by XNORing random pairs of image hashes and setting the appropriate target variable (1 if same meme, else 0)\n",
    "* Model is trained (currently at about 98% precision and recall)\n",
    "* The meme group classification algorithm using a the trained model while comparing memes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meme_id</th>\n",
       "      <th>data_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>template</th>\n",
       "      <th>week</th>\n",
       "      <th>phash_8</th>\n",
       "      <th>phash_8_resized</th>\n",
       "      <th>phash_32_resized</th>\n",
       "      <th>phash_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>353mvs</td>\n",
       "      <td>189966232</td>\n",
       "      <td>2</td>\n",
       "      <td>Emma Watson Troll</td>\n",
       "      <td>2011-9-3</td>\n",
       "      <td>cc24f3b324a34f39</td>\n",
       "      <td>cc2df3b306b24e38</td>\n",
       "      <td>ccc72ccd0d0a7caff3107cc1b3266c8d04fb70a7f2552d...</td>\n",
       "      <td>ccc714e724164ca2f3105587b3365c8634db2ce7a36145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353mw1</td>\n",
       "      <td>189966241</td>\n",
       "      <td>3</td>\n",
       "      <td>Emma Watson Troll</td>\n",
       "      <td>2011-9-3</td>\n",
       "      <td>cc2cf3b396a14e38</td>\n",
       "      <td>cc2df3b326914f18</td>\n",
       "      <td>cce0147e2d8b4188f3105474b323564c26f360e491691f...</td>\n",
       "      <td>ccc1106d2c485b20f310534cb3325a4496fb28e4a17511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>353mw5</td>\n",
       "      <td>189966245</td>\n",
       "      <td>445</td>\n",
       "      <td>The Most Interesting Man In The World</td>\n",
       "      <td>2011-9-3</td>\n",
       "      <td>cd06f0b9b8e54a5a</td>\n",
       "      <td>cd40f199b0f74a5a</td>\n",
       "      <td>cd5d997540278689f100017599ff3e21b3744cddf73038...</td>\n",
       "      <td>cd7fe9ab0617cfd3f1000550b9ff7020b8544ed8e53824...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>353mxa</td>\n",
       "      <td>189966286</td>\n",
       "      <td>39</td>\n",
       "      <td>Irrational Shaggy</td>\n",
       "      <td>2011-9-3</td>\n",
       "      <td>9cb2e4d1dfd21a05</td>\n",
       "      <td>9cb2e4d8dad21b85</td>\n",
       "      <td>9c41e9c93319a613e480e69bd0266689da119133d264e6...</td>\n",
       "      <td>9c45098b12588713e480441bd026018dca3184b4d244b6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>353my8</td>\n",
       "      <td>189966320</td>\n",
       "      <td>1383</td>\n",
       "      <td>Redditors Wife</td>\n",
       "      <td>2011-9-3</td>\n",
       "      <td>98a3c55bb62e654c</td>\n",
       "      <td>98a3c51ba63e654d</td>\n",
       "      <td>988c86f1a3ffd670c5d884ed1b991ed0a66626c12eee13...</td>\n",
       "      <td>98882731a3ffcc78c5f888cc5b991cdcb6632f802ecc1b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meme_id    data_id  rating                               template      week  \\\n",
       "0  353mvs  189966232       2                      Emma Watson Troll  2011-9-3   \n",
       "1  353mw1  189966241       3                      Emma Watson Troll  2011-9-3   \n",
       "2  353mw5  189966245     445  The Most Interesting Man In The World  2011-9-3   \n",
       "3  353mxa  189966286      39                      Irrational Shaggy  2011-9-3   \n",
       "4  353my8  189966320    1383                         Redditors Wife  2011-9-3   \n",
       "\n",
       "            phash_8   phash_8_resized  \\\n",
       "0  cc24f3b324a34f39  cc2df3b306b24e38   \n",
       "1  cc2cf3b396a14e38  cc2df3b326914f18   \n",
       "2  cd06f0b9b8e54a5a  cd40f199b0f74a5a   \n",
       "3  9cb2e4d1dfd21a05  9cb2e4d8dad21b85   \n",
       "4  98a3c55bb62e654c  98a3c51ba63e654d   \n",
       "\n",
       "                                    phash_32_resized  \\\n",
       "0  ccc72ccd0d0a7caff3107cc1b3266c8d04fb70a7f2552d...   \n",
       "1  cce0147e2d8b4188f3105474b323564c26f360e491691f...   \n",
       "2  cd5d997540278689f100017599ff3e21b3744cddf73038...   \n",
       "3  9c41e9c93319a613e480e69bd0266689da119133d264e6...   \n",
       "4  988c86f1a3ffd670c5d884ed1b991ed0a66626c12eee13...   \n",
       "\n",
       "                                            phash_32  \n",
       "0  ccc714e724164ca2f3105587b3365c8634db2ce7a36145...  \n",
       "1  ccc1106d2c485b20f310534cb3325a4496fb28e4a17511...  \n",
       "2  cd7fe9ab0617cfd3f1000550b9ff7020b8544ed8e53824...  \n",
       "3  9c45098b12588713e480441bd026018dca3184b4d244b6...  \n",
       "4  98882731a3ffcc78c5f888cc5b991cdcb6632f802ecc1b...  "
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('memes_total_time.csv')\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "df = df.groupby('meme_id').agg('first').reset_index()\n",
    "\n",
    "# Fixing some misclassified memes that my model found later in this notebook\n",
    "df.loc[df['template'].isin(['Bad Time', 'Youre gonna have a bad time']), 'template'] = 'Bad Time'\n",
    "df.loc[df['template'].isin(['Annoyed Picard', 'Annoyed Picard HD']), 'template'] = 'Annoyed Picard'\n",
    "df.loc[df['template'].isin(['Boromir', 'Boromirmod']), 'template'] = 'Boromir'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Cleaning Images\n",
    "\n",
    "You shouldn't need to run any of this code again. It takes a long time anyway.\n",
    "\n",
    "This section involves taking the dataframe (which originally had no image data in it) and fetching the relevant meme data from the internet. This involved using the requests library and scraping the image bytes manually. I did a bit of cleaning by deleting empty downloads from dead links and images that for some reason became corrupt. Only a few thousand had issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes multiple hours to run. No need to run as the Images are all in IMAGES folder.\n",
    "# This is just here for formality.\n",
    "\n",
    "def scrape_image(meme_id):\n",
    "    url =  'http://i.qkme.me/' + meme_id + '.jpg'\n",
    "    \n",
    "    try:\n",
    "        req = requests.get(url)\n",
    "        image = Image.open(BytesIO(req.content))\n",
    "        image.save('IMAGES/' + meme_id + '.jpg', 'jpeg')\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for meme_id in df['meme_id']:\n",
    "    scrape_image(meme_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block removes corrputed images. i.e. images that have size 0.\n",
    "\n",
    "image_folder = os.path.join(os.getcwd(), 'IMAGES')\n",
    "image_paths = [os.path.join(image_folder, x) for x in list(os.walk(image_folder))[0][2]]\n",
    "\n",
    "num_corrputed = 0\n",
    "for path in image_paths:\n",
    "    if os.stat(path).st_size:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        num_corrputed += 1\n",
    "        os.remove(path)\n",
    "        \n",
    "print(f'deleted {num_corrputed} corrupted files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block resized images to 48x48 pixels. These smaller images are used when generating hashes\n",
    "# Estimated Runtime: 50 minutes\n",
    "\n",
    "# Reaccessing image directory for safe measure sice we modified its contents\n",
    "image_folder = os.path.join(os.getcwd(), 'IMAGES')\n",
    "image_paths = [os.path.join(image_folder, x) for x in list(os.walk(image_folder))[0][2]\n",
    "                   if x.endswith('.jpg')]\n",
    "\n",
    "\n",
    "for ind, path in enumerate(image_paths):\n",
    "    im = Image.open(path)\n",
    "    resized_im = im.resize((48, 48))\n",
    "    resized_im.save(os.path.join(os.getcwd(), 'IMAGES_RESIZED', os.path.split(path)[-1]))\n",
    "        \n",
    "resized_image_folder = os.path.join(os.getcwd(), 'IMAGES_RESIZED')\n",
    "resized_image_paths = [os.path.join(resized_image_folder, x) for x in list(os.walk(resized_image_folder))[0][2]\n",
    "                   if x.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Hashes\n",
    "\n",
    "This is where I created the perceptual hashes for each image. I created  8-bit and 32-bit hashes for both fullsized and 48x48 sized images. This was done to see give me a variety in hash data and see if the size of the image or bit-size mattered. It turned out that there wasn't much of a difference in performance over the different types of hashes so I stuck with the 8-bit hashes of the original images (phash_8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_meme(meme, resized = False):\n",
    "    \"\"\"Useful for opening memes in the IMAGES directory.\n",
    "       Use the resized keyword if you want the smaller resized version.\n",
    "       \n",
    "       Input should be the 'meme_id' value in the dataframe, e.g. '353mw1'.\"\"\"\n",
    "    \n",
    "    path = os.path.join(os.getcwd(), 'IMAGES' + ('_RESIZED' if resized else ''), meme + '.jpg')\n",
    "    return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 1:15 to run. Just run this once and save the data to your source CSV.\n",
    "\n",
    "for resize in [True, False]:\n",
    "    for size in [8, 32]:\n",
    "        df['phash_{}'.format(str(size) + ('_resized' if resize else ''))] = \\\n",
    "        df['meme_id'].apply(lambda x: imagehash.phash(open_meme(x), hash_size = 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "Here, I create a sequential Keras model and train it to tell two images apart. This is done by XNORing their respective hashes and feeding those bits into the model as features. The target variable is 1 if they are the same meme and 0 otherwise. There's obviously a heavy class inbalance when it comes to training but that didn't affect the model much in terms of precision/recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_bits(string_hash):\n",
    "    \"\"\" Takes a series of hex bits and returns the string of binary values that it corresponds to.\n",
    "        e.g. 'cc2df3b306b24e38' --> '1100110000101101111100111011001100000110101100100100111000111000'\n",
    "    \"\"\"\n",
    "    return BitArray(bytes.fromhex(string_hash)).bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  \\\n",
       "42   1  1  0  0  1  1  0  0  1  0   0   0   1   1   1   0   1   0   1   1   1   \n",
       "108  1  1  0  0  0  0  0  1  0  0   0   0   0   1   0   0   1   1   1   1   1   \n",
       "110  1  0  0  1  0  0  0  0  0  1   0   1   0   0   0   0   0   1   1   1   1   \n",
       "137  1  0  0  1  1  0  1  1  0  0   0   0   0   1   0   1   1   1   0   0   0   \n",
       "154  1  1  1  0  1  0  1  0  1  0   0   0   0   1   0   1   1   0   0   1   0   \n",
       "\n",
       "     21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  \\\n",
       "42    0   0   1   0   0   1   1   0   0   1   1   1   0   1   0   0   1   0   \n",
       "108   1   1   1   0   0   0   1   1   1   0   1   0   1   0   1   0   1   1   \n",
       "110   1   0   1   1   1   1   0   0   1   0   1   0   1   0   1   1   0   1   \n",
       "137   1   1   0   0   1   1   0   0   0   1   0   0   0   1   0   1   1   0   \n",
       "154   1   0   1   0   1   1   1   1   0   1   0   1   0   0   0   0   1   0   \n",
       "\n",
       "     39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  \\\n",
       "42    0   1   1   1   1   0   0   1   0   0   0   1   1   1   0   0   1   0   \n",
       "108   1   1   0   0   0   0   1   1   1   1   1   1   1   0   0   0   0   0   \n",
       "110   0   1   1   1   0   0   0   1   1   0   0   1   1   0   0   0   0   1   \n",
       "137   0   1   1   0   1   1   0   1   1   1   0   1   1   0   1   0   0   1   \n",
       "154   1   1   1   0   0   1   1   0   1   0   1   1   0   1   0   1   0   1   \n",
       "\n",
       "     57  58  59  60  61  62  63  target  \n",
       "42    0   0   0   1   0   1   1       1  \n",
       "108   0   1   1   1   0   0   0       5  \n",
       "110   0   1   1   1   0   1   1       0  \n",
       "137   1   1   0   1   1   0   0       2  \n",
       "154   0   0   0   0   0   1   1       6  "
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = np.random.choice(df.groupby('template').size().sort_values(ascending = False).index[:100], 10, replace = False)\n",
    "\n",
    "test_df = df.loc[df['template'].isin(top_5), 'phash_8'].apply(lambda x: pd.Series([int(x) for x in BitArray(bytes.fromhex(str(x))).bin]))\n",
    "\n",
    "template_le = LabelEncoder()\n",
    "template_le.fit(df.loc[df['template'].isin(top_5), 'template'])\n",
    "test_df['target'] = template_le.transform(df.loc[df['template'].isin(top_5), 'template'])\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might take a minute or two to run\n",
    "# sample 1000 hashes and use them for training. This generates n^2 samples from n images. So we train on 1,000,000 hashes.\n",
    "training_source = df[['template', 'phash_8']].sample(1000)\n",
    "training_source['phash_8'] = training_source['phash_8'].apply(hash_to_bits)\n",
    "\n",
    "training_xnors = []\n",
    "target = []\n",
    "\n",
    "# Essentially code to turn a string of 64 bits into a 64-featured dataframe.\n",
    "for _, (template_1, hash_1) in source.iterrows():\n",
    "    for _, (template_2, hash_2) in source.iterrows():\n",
    "        training_xnors.append([int(hash_1[ind] == hash_2[ind]) for ind in range(len(hash_1))])\n",
    "        target.append(template_1 == template_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  \\\n",
       "0  1  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   \n",
       "1  1  1  0  0  1  0  0  0  1  1   1   0   0   1   0   1   0   0   0   1   1   \n",
       "2  1  1  1  1  1  0  1  1  1  1   1   1   0   1   1   1   1   1   0   0   1   \n",
       "3  1  0  1  0  1  0  0  1  1  0   1   1   0   1   0   1   0   1   1   0   1   \n",
       "4  1  1  1  1  0  1  1  1  1  0   0   0   1   1   0   1   0   1   0   1   1   \n",
       "\n",
       "   21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  \\\n",
       "0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   \n",
       "1   0   0   0   1   0   1   1   0   1   0   0   0   1   1   1   0   1   1   0   \n",
       "2   0   1   1   0   1   1   0   1   0   1   1   0   1   1   1   0   0   1   0   \n",
       "3   0   1   0   0   1   0   1   1   1   1   1   1   1   0   1   0   1   0   0   \n",
       "4   0   1   0   0   0   0   0   0   1   0   1   1   1   0   1   0   0   0   0   \n",
       "\n",
       "   40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  \\\n",
       "0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   \n",
       "1   0   1   0   0   0   0   0   0   1   1   0   1   0   1   1   1   1   1   0   \n",
       "2   0   1   0   0   1   1   1   1   0   1   1   1   1   1   0   1   1   0   1   \n",
       "3   1   1   1   0   0   0   1   1   0   1   1   1   1   0   0   0   1   1   1   \n",
       "4   0   1   0   1   1   0   0   1   1   0   1   1   0   1   0   0   1   0   0   \n",
       "\n",
       "   59  60  61  62  63  target  \n",
       "0   1   1   1   1   1       1  \n",
       "1   1   0   0   0   1       0  \n",
       "2   1   0   1   0   1       0  \n",
       "3   0   1   0   1   1       0  \n",
       "4   0   0   1   0   0       0  "
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also takes a minute or two to run\n",
    "test_df = pd.Series(training_xnors).apply(pd.Series)\n",
    "test_df['target'] = pd.Series(target).astype(int)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into 75% train 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_df[test_df.columns[:-1]], test_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model. \n",
    "# 64 layer -> 100 layer -> (30% dropout) -> 100 layer -> 10 layer -> binary output\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=64, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Stop the model when binary crossentropy stops improving\n",
    "es = EarlyStopping(monitor = 'loss', min_delta = .0000001, patience = 5, \n",
    "                   verbose = 0, mode ='auto', restore_best_weights = False)\n",
    "\n",
    "# Adamax had best performance over other keras options\n",
    "model.compile(optimizer='Adamax',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "750000/750000 [==============================] - 6s 9us/step - loss: 0.0579 - binary_accuracy: 0.9909\n",
      "Epoch 2/500\n",
      "750000/750000 [==============================] - 3s 5us/step - loss: 0.0138 - binary_accuracy: 0.9942\n",
      "Epoch 3/500\n",
      "750000/750000 [==============================] - 3s 5us/step - loss: 0.0091 - binary_accuracy: 0.9942\n",
      "Epoch 4/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0075 - binary_accuracy: 0.9942\n",
      "Epoch 5/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0067 - binary_accuracy: 0.9942\n",
      "Epoch 6/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0063 - binary_accuracy: 0.9942\n",
      "Epoch 7/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0060 - binary_accuracy: 0.9942\n",
      "Epoch 8/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0058 - binary_accuracy: 0.9967\n",
      "Epoch 9/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0056 - binary_accuracy: 0.9988\n",
      "Epoch 10/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0054 - binary_accuracy: 0.9989\n",
      "Epoch 11/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0052 - binary_accuracy: 0.9990\n",
      "Epoch 12/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0051 - binary_accuracy: 0.9991\n",
      "Epoch 13/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0050 - binary_accuracy: 0.9992\n",
      "Epoch 14/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0049 - binary_accuracy: 0.9992\n",
      "Epoch 15/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0047 - binary_accuracy: 0.9992\n",
      "Epoch 16/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0047 - binary_accuracy: 0.9993\n",
      "Epoch 17/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0045 - binary_accuracy: 0.9993\n",
      "Epoch 18/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0044 - binary_accuracy: 0.9994\n",
      "Epoch 19/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0043 - binary_accuracy: 0.9994\n",
      "Epoch 20/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0042 - binary_accuracy: 0.9994\n",
      "Epoch 21/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0041 - binary_accuracy: 0.9995\n",
      "Epoch 22/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0040 - binary_accuracy: 0.9995\n",
      "Epoch 23/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0039 - binary_accuracy: 0.9995\n",
      "Epoch 24/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0038 - binary_accuracy: 0.9996\n",
      "Epoch 25/500\n",
      "750000/750000 [==============================] - 4s 6us/step - loss: 0.0036 - binary_accuracy: 0.9995A: 1s - loss: 0.0036 - binary\n",
      "Epoch 26/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0035 - binary_accuracy: 0.9996\n",
      "Epoch 27/500\n",
      "750000/750000 [==============================] - 3s 5us/step - loss: 0.0033 - binary_accuracy: 0.9996\n",
      "Epoch 28/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0032 - binary_accuracy: 0.9996\n",
      "Epoch 29/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0031 - binary_accuracy: 0.9996\n",
      "Epoch 30/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0029 - binary_accuracy: 0.9996\n",
      "Epoch 31/500\n",
      "750000/750000 [==============================] - 3s 5us/step - loss: 0.0028 - binary_accuracy: 0.9996\n",
      "Epoch 32/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0026 - binary_accuracy: 0.9996\n",
      "Epoch 33/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0024 - binary_accuracy: 0.9996\n",
      "Epoch 34/500\n",
      "750000/750000 [==============================] - 4s 6us/step - loss: 0.0023 - binary_accuracy: 0.9996\n",
      "Epoch 35/500\n",
      "750000/750000 [==============================] - 4s 6us/step - loss: 0.0022 - binary_accuracy: 0.9996\n",
      "Epoch 36/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0020 - binary_accuracy: 0.9996\n",
      "Epoch 37/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0019 - binary_accuracy: 0.9996\n",
      "Epoch 38/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0018 - binary_accuracy: 0.9997\n",
      "Epoch 39/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0017 - binary_accuracy: 0.9997\n",
      "Epoch 40/500\n",
      "750000/750000 [==============================] - 3s 5us/step - loss: 0.0017 - binary_accuracy: 0.9997\n",
      "Epoch 41/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0016 - binary_accuracy: 0.9996\n",
      "Epoch 42/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0016 - binary_accuracy: 0.9997\n",
      "Epoch 43/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0015 - binary_accuracy: 0.9997\n",
      "Epoch 44/500\n",
      "750000/750000 [==============================] - 3s 4us/step - loss: 0.0015 - binary_accuracy: 0.9997\n",
      "Epoch 45/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0014 - binary_accuracy: 0.9997\n",
      "Epoch 46/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0014 - binary_accuracy: 0.9997\n",
      "Epoch 47/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0014 - binary_accuracy: 0.9997\n",
      "Epoch 48/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0013 - binary_accuracy: 0.9997\n",
      "Epoch 49/500\n",
      "750000/750000 [==============================] - 4s 6us/step - loss: 0.0013 - binary_accuracy: 0.9997\n",
      "Epoch 50/500\n",
      "750000/750000 [==============================] - 4s 6us/step - loss: 0.0013 - binary_accuracy: 0.9997\n",
      "Epoch 51/500\n",
      "750000/750000 [==============================] - 4s 5us/step - loss: 0.0013 - binary_accuracy: 0.9997\n",
      "Epoch 52/500\n",
      "750000/750000 [==============================] - 4s 6us/step - loss: 0.0013 - binary_accuracy: 0.9997\n",
      "Epoch 53/500\n",
      "750000/750000 [==============================] - 5s 7us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 54/500\n",
      "750000/750000 [==============================] - 6s 8us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 55/500\n",
      "750000/750000 [==============================] - 6s 9us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 56/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 57/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 58/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 59/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 60/500\n",
      "750000/750000 [==============================] - 9s 13us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 61/500\n",
      "750000/750000 [==============================] - 19s 25us/step - loss: 0.0011 - binary_accuracy: 0.9997\n",
      "Epoch 62/500\n",
      "750000/750000 [==============================] - 26s 34us/step - loss: 0.0012 - binary_accuracy: 0.9997 9s - loss: 0.0012 - binary_accuracy \n",
      "Epoch 63/500\n",
      "750000/750000 [==============================] - 29s 38us/step - loss: 0.0011 - binary_accuracy: 0.9997 10s - loss: - ETA: 4s - loss: 0.0011 - binary_a - ETA: 2s - loss: 0.0011 - binary_accur\n",
      "Epoch 64/500\n",
      "750000/750000 [==============================] - 23s 31us/step - loss: 0.0011 - binary_accuracy: 0.9997\n",
      "Epoch 65/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 0.0012 - binary_accuracy: 0.9997\n",
      "Epoch 66/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 67/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 68/500\n",
      "750000/750000 [==============================] - 5s 7us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 69/500\n",
      "750000/750000 [==============================] - 9s 12us/step - loss: 0.0011 - binary_accuracy: 0.9998: 0s - loss: 0.0011 - binary_accuracy: 0.99\n",
      "Epoch 70/500\n",
      "750000/750000 [==============================] - 10s 13us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 71/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 72/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 0.0010 - binary_accuracy: 0.9998\n",
      "Epoch 73/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 74/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 0.0010 - binary_accuracy: 0.9998\n",
      "Epoch 75/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 0.0010 - binary_accuracy: 0.9998\n",
      "Epoch 76/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 0.0010 - binary_accuracy: 0.9998\n",
      "Epoch 77/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 0.0011 - binary_accuracy: 0.9998\n",
      "Epoch 78/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 0.0010 - binary_accuracy: 0.9998\n",
      "Epoch 79/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 0.0010 - binary_accuracy: 0.9998\n",
      "Epoch 80/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 9.9192e-04 - binary_accuracy: 0.9998\n",
      "Epoch 81/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 9.9804e-04 - binary_accuracy: 0.9998\n",
      "Epoch 82/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 9.9738e-04 - binary_accuracy: 0.9998\n",
      "Epoch 83/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 9.8472e-04 - binary_accuracy: 0.9998\n",
      "Epoch 84/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 9.6211e-04 - binary_accuracy: 0.9998\n",
      "Epoch 85/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 9.4196e-04 - binary_accuracy: 0.9998\n",
      "Epoch 86/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 9.4317e-04 - binary_accuracy: 0.9998\n",
      "Epoch 87/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 9.4151e-04 - binary_accuracy: 0.9998: 0s - loss: 9.3081e-04 - binary_accuracy: 0.99\n",
      "Epoch 88/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 9.5052e-04 - binary_accuracy: 0.9998\n",
      "Epoch 89/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 8.9954e-04 - binary_accuracy: 0.9998\n",
      "Epoch 90/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 9.5163e-04 - binary_accuracy: 0.9998\n",
      "Epoch 91/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.9576e-04 - binary_accuracy: 0.9998\n",
      "Epoch 92/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 8.9948e-04 - binary_accuracy: 0.9998\n",
      "Epoch 93/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 9.0547e-04 - binary_accuracy: 0.9998\n",
      "Epoch 94/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.8492e-04 - binary_accuracy: 0.9998\n",
      "Epoch 95/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 8.8231e-04 - binary_accuracy: 0.9998\n",
      "Epoch 96/500\n",
      "750000/750000 [==============================] - 9s 11us/step - loss: 8.9408e-04 - binary_accuracy: 0.9998\n",
      "Epoch 97/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 8.8395e-04 - binary_accuracy: 0.9998\n",
      "Epoch 98/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 8.6432e-04 - binary_accuracy: 0.9998\n",
      "Epoch 99/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 8.7259e-04 - binary_accuracy: 0.9998\n",
      "Epoch 100/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 9.0779e-04 - binary_accuracy: 0.9998\n",
      "Epoch 101/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.2592e-04 - binary_accuracy: 0.9998\n",
      "Epoch 102/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.4555e-04 - binary_accuracy: 0.9998\n",
      "Epoch 103/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.3231e-04 - binary_accuracy: 0.9998\n",
      "Epoch 104/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.2850e-04 - binary_accuracy: 0.9998\n",
      "Epoch 105/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 8.0587e-04 - binary_accuracy: 0.9998\n",
      "Epoch 106/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 8.2062e-04 - binary_accuracy: 0.9998\n",
      "Epoch 107/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 8.2264e-04 - binary_accuracy: 0.9998\n",
      "Epoch 108/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 7.9145e-04 - binary_accuracy: 0.9998\n",
      "Epoch 109/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 8.1296e-04 - binary_accuracy: 0.9998\n",
      "Epoch 110/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 8.2405e-04 - binary_accuracy: 0.9998\n",
      "Epoch 111/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 7.8335e-04 - binary_accuracy: 0.9998\n",
      "Epoch 112/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 7.6709e-04 - binary_accuracy: 0.9998A: 0s - loss: 7.4120e-04 - binary_accuracy: 0.9\n",
      "Epoch 113/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 7.8099e-04 - binary_accuracy: 0.9998\n",
      "Epoch 114/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 7.7542e-04 - binary_accuracy: 0.9998\n",
      "Epoch 115/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 7.5780e-04 - binary_accuracy: 0.9998\n",
      "Epoch 116/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 7.3305e-04 - binary_accuracy: 0.9998\n",
      "Epoch 117/500\n",
      "750000/750000 [==============================] - 9s 12us/step - loss: 7.3778e-04 - binary_accuracy: 0.9999\n",
      "Epoch 118/500\n",
      "750000/750000 [==============================] - 10s 13us/step - loss: 7.3546e-04 - binary_accuracy: 0.9998 0s - loss: 7.3641e-04 - binary_accuracy: 0.9\n",
      "Epoch 119/500\n",
      "750000/750000 [==============================] - 9s 12us/step - loss: 7.3102e-04 - binary_accuracy: 0.9998\n",
      "Epoch 120/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 7.1514e-04 - binary_accuracy: 0.9999\n",
      "Epoch 121/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 7.3312e-04 - binary_accuracy: 0.9998\n",
      "Epoch 122/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 6.7805e-04 - binary_accuracy: 0.9999\n",
      "Epoch 123/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 7.0704e-04 - binary_accuracy: 0.9998: 2s - l\n",
      "Epoch 124/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 6.6789e-04 - binary_accuracy: 0.9999\n",
      "Epoch 125/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 6.4647e-04 - binary_accuracy: 0.9999\n",
      "Epoch 126/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 6.3464e-04 - binary_accuracy: 0.9999\n",
      "Epoch 127/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 6.2444e-04 - binary_accuracy: 0.9999\n",
      "Epoch 128/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 6.3766e-04 - binary_accuracy: 0.9999\n",
      "Epoch 129/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 6.1787e-04 - binary_accuracy: 0.9999\n",
      "Epoch 130/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 6.4746e-04 - binary_accuracy: 0.9999\n",
      "Epoch 131/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.8861e-04 - binary_accuracy: 0.9999\n",
      "Epoch 132/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.8998e-04 - binary_accuracy: 0.9999\n",
      "Epoch 133/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 5.9589e-04 - binary_accuracy: 0.9999\n",
      "Epoch 134/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.9262e-04 - binary_accuracy: 0.9999\n",
      "Epoch 135/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 5.6942e-04 - binary_accuracy: 0.9999\n",
      "Epoch 136/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.2288e-04 - binary_accuracy: 0.9999\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.4945e-04 - binary_accuracy: 0.9999\n",
      "Epoch 138/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.6217e-04 - binary_accuracy: 0.9999\n",
      "Epoch 139/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.1715e-04 - binary_accuracy: 0.9999\n",
      "Epoch 140/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 5.2384e-04 - binary_accuracy: 0.9999\n",
      "Epoch 141/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.1247e-04 - binary_accuracy: 0.9999\n",
      "Epoch 142/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 5.0622e-04 - binary_accuracy: 0.9999\n",
      "Epoch 143/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.9837e-04 - binary_accuracy: 0.9999\n",
      "Epoch 144/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.8068e-04 - binary_accuracy: 0.9999\n",
      "Epoch 145/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 4.8257e-04 - binary_accuracy: 0.9999\n",
      "Epoch 146/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.9496e-04 - binary_accuracy: 0.9999\n",
      "Epoch 147/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 5.1400e-04 - binary_accuracy: 0.9999\n",
      "Epoch 148/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 4.4713e-04 - binary_accuracy: 0.9999\n",
      "Epoch 149/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.6416e-04 - binary_accuracy: 0.9999\n",
      "Epoch 150/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 4.6544e-04 - binary_accuracy: 0.9999\n",
      "Epoch 151/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 5.0611e-04 - binary_accuracy: 0.9999\n",
      "Epoch 152/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 4.4512e-04 - binary_accuracy: 0.9999\n",
      "Epoch 153/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 4.4802e-04 - binary_accuracy: 0.9999\n",
      "Epoch 154/500\n",
      "750000/750000 [==============================] - 7s 9us/step - loss: 4.3122e-04 - binary_accuracy: 0.9999\n",
      "Epoch 155/500\n",
      "750000/750000 [==============================] - 8s 10us/step - loss: 4.4332e-04 - binary_accuracy: 0.9999: 2s - loss: 4.0233e-04 - \n",
      "Epoch 156/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.9004e-04 - binary_accuracy: 0.9999\n",
      "Epoch 157/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.4934e-04 - binary_accuracy: 0.9999\n",
      "Epoch 158/500\n",
      "750000/750000 [==============================] - 8s 11us/step - loss: 4.4309e-04 - binary_accuracy: 0.9999\n",
      "Epoch 159/500\n",
      "750000/750000 [==============================] - 7s 10us/step - loss: 4.3467e-04 - binary_accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1731740b8>"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usually takes ~200 epochs for loss to converge\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 5000, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000/250000 [==============================] - 1s 2us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[248494     30]\n",
      " [    32   1444]]\n",
      "0.999752\n",
      "0.9796472184531886\n",
      "0.978319783197832\n"
     ]
    }
   ],
   "source": [
    "# This confusion matrix shows that there were only 62 misdclassifications out of 250,000 test cases.\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size = 1000)\n",
    "preds = [round(x[0]) for x in model.predict(X_test)] #model.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test.to_list(), preds)\n",
    "normalized_cm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(normalized_cm, cmap = 'Blues') # a heatmap normalized by  class\n",
    "plt.show()\n",
    "print(conf_mat)\n",
    "\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(precision_score(y_test, preds))\n",
    "print(recall_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meme Classifier\n",
    "\n",
    "Now that the model is trained, here I attempted to use it to distinguish different groups of memes. This was dont by iteratively asking the same question: \"Are these two memes the same?\". This worked okay but a lot of improvements can be made here. I tried other things such as DBSCAN and KNN but this method was much faster and gave better results. The really relevant code is in the \"classify_memes\" method. It's fairly straight-forward how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeClassifier():\n",
    "    \"\"\" This is the class that manages multiple groups of memes. It keeps track of multiple instances of\n",
    "        the MemeGroup class. This purpose of this class is to compare Memes and group them.\"\"\"\n",
    "    def __init__(self, memes, meme_groups = [], hash_size = 8, resized = True):\n",
    "        self.memes = memes\n",
    "        self.meme_groups = meme_groups or [MemeGroup([meme]) for meme in self.memes]\n",
    "        self.hash_dict = self._build_hash_dict(hash_size, resized)\n",
    "        self.max_classification_rounds = 10\n",
    "        \n",
    "    def _build_hash_dict(self, hash_size = 8, resized = True):\n",
    "        print('initializing hash dictionary')\n",
    "        meme_paths = {meme: os.path.join(os.getcwd(), 'IMAGES' + ('_RESIZED' if resized else ''), meme + '.jpg')\n",
    "                      for meme in self.memes}\n",
    "        hash_dict = {meme: imagehash.phash(Image.open(meme_paths[meme]), hash_size = hash_size) for meme in self.memes}\n",
    "        \n",
    "        print('Done')\n",
    "        \n",
    "        return hash_dict\n",
    "            \n",
    "        \n",
    "    def save_classifications(self, path = 'meme_classifications.csv'):\n",
    "        meme_dict = {'memes': {group.group_id: group.memes for group in self.meme_groups}}\n",
    "        meme_df = pd.DataFrame.from_dict(meme_save).to_csv(path)\n",
    "        \n",
    "        print('Done')\n",
    "        \n",
    "    \n",
    "    def load_classifications(self, path = 'meme_classification.txt'):\n",
    "        # Might need to rework as UUIDs wont take into account UUIDs generated in previous instances.\n",
    "        \n",
    "        meme_dict = pd.read_csv(path).rename({'Unnamed: 0': 'id'}, axis = 1).to_dict(orient = 'index')\n",
    "        \n",
    "        for group in list(meme_dict.values())[:1]:\n",
    "            mg = MemeGroup(memes = group['memes'], group_id = group['id'])\n",
    "            self.meme_groups.append(mg)\n",
    "        \n",
    "        print('Done')\n",
    "    \n",
    "        \n",
    "        \n",
    "    def is_same_meme(self, meme_1, meme_2, prob = False):\n",
    "        hash_1 = hash_dict[meme_1]\n",
    "        hash_2 = hash_dict[meme_2]\n",
    "        xnor = [1 if hash_1[i] == hash_2[i] else 0 for i in range(len(hash_1))]\n",
    "\n",
    "        result = model.predict_proba(np.array([xnor]))[0][0] if prob else round(model.predict(np.array([xnor]))[0][0])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def is_same_meme_group(self, group_x, group_y, evidence):\n",
    "        \n",
    "        min_evidence = min(len(group_x.memes), len(group_y.memes), evidence)\n",
    "        rounds = 0\n",
    "        num_pos = 0\n",
    "        \n",
    "        while rounds < min_evidence:\n",
    "            if self.is_same_meme(np.random.choice(group_x.memes), np.random.choice(group_y.memes)):\n",
    "                num_pos += 1\n",
    "                \n",
    "            rounds += 1\n",
    "            \n",
    "        return num_pos == min_evidence\n",
    "    \n",
    "    \n",
    "    def merge_groups(self, group_ids):\n",
    "        groups = [group for group in self.meme_groups if group.group_id in group_ids]\n",
    "        new_memes = [meme for group in groups for meme in group.memes]\n",
    "        new_clustering_rounds = max([group.clustering_rounds for group in groups])\n",
    "        \n",
    "        new_group = MemeGroup(memes = new_memes, clustering_rounds = new_clustering_rounds)\n",
    "        \n",
    "        for group in groups:\n",
    "            self.meme_groups.remove(group)\n",
    "            del(group)\n",
    "            \n",
    "        self.meme_groups.append(new_group)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def classify_memes(self, min_group_size = 10, evidence = 5):\n",
    "        # Evidence is the number of memes, meme_y, in a group that some meme, meme_x must be \"the same meme\" with\n",
    "        # in order to be accepted into the group. In reality evidence can't be larger than the group size.\n",
    "        rounds = 0 # The number of times the algorithm has passsed through every meme\n",
    "        num_previous_groups = np.inf # Number of groups in last pass. If this is the same twice, then break\n",
    "        num_current_groups = len(self.meme_groups)\n",
    "        \n",
    "        while (any([1 for group in self.meme_groups if group.size < min_group_size]) and \n",
    "               rounds < self.max_classification_rounds and\n",
    "               num_previous_groups > num_current_groups):\n",
    "            \n",
    "            for group_x in self.meme_groups:\n",
    "                groups_to_merge = []\n",
    "\n",
    "                for group_y in self.meme_groups: \n",
    "                    if group_y != group_x:\n",
    "                        if self.is_same_meme_group(group_x, group_y, evidence):\n",
    "                            groups_to_merge.append(group_y.group_id)\n",
    "\n",
    "\n",
    "                if groups_to_merge:\n",
    "                    self.merge_groups(groups_to_merge + [group_x.group_id])\n",
    "                \n",
    "            print(f'Classified memes into {len(self.meme_groups)} categories.' + \n",
    "                  (' Reitterating.' if min([group.size for group in self.meme_groups]) < min_group_size else ''))\n",
    "            \n",
    "            rounds += 1\n",
    "            num_previous_groups = num_current_groups\n",
    "            num_current_groups = len(self.meme_groups)\n",
    "            \n",
    "                \n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeGroup():\n",
    "    \"\"\" This is the class that every group of memes takes on. The group has an ID, size, etc. The meme_id's found in\n",
    "        the dataframe are what is stored in the memes attribute.\"\"\"\n",
    "    def __init__(self, memes = [], group_id = None, clustering_rounds = 0):\n",
    "        self.memes = memes\n",
    "        self.size = len(memes)\n",
    "        self.clustering_rounds = clustering_rounds\n",
    "        self.group_id = group_id or str(uuid.uuid1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of 50 meme groups with a high number of samples in the dataframe\n",
    "random_20_memes = df.groupby('template').size().sort_values(ascending = False)[:100].sample(20).index\n",
    "meme_ids = df.loc[df['template'].isin(random_20_memes), 'meme_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21485\n"
     ]
    }
   ],
   "source": [
    "# This is how many memes are in the 20 given meme groups to be classified.\n",
    "print(len(meme_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing hash dictionary\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# initialize a classifier on all of the unique meme id's in our dataframe (170,000 + of them)\n",
    "mc = MemeClassifier(memes = meme_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified memes into 134 categories. Reitterating.\n",
      "Classified memes into 86 categories. Reitterating.\n",
      "Classified memes into 80 categories. Reitterating.\n",
      "Classified memes into 73 categories. Reitterating.\n",
      "Classified memes into 72 categories. Reitterating.\n",
      "Classified memes into 69 categories. Reitterating.\n",
      "Classified memes into 68 categories. Reitterating.\n",
      "Classified memes into 67 categories. Reitterating.\n",
      "Classified memes into 66 categories. Reitterating.\n",
      "Classified memes into 66 categories. Reitterating.\n",
      "Done\n",
      "590\n"
     ]
    }
   ],
   "source": [
    "#runtime: 16 hours on full dataframe\n",
    "t1 = datetime.utcnow()\n",
    "mc.classify_memes(min_group_size = 5, evidence = 5)\n",
    "t2 = datetime.utcnow()\n",
    "\n",
    "print((t2 - t1).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Ron Paul'},\n",
       " {'Ron Paul'},\n",
       " {'Ron Paul'},\n",
       " {'Ron Paul'},\n",
       " {'One Does Not Simply'},\n",
       " {'Scumbag Genetics'},\n",
       " {'Ron Paul'},\n",
       " {'Ron Paul'},\n",
       " {'Bad Luck Brian'},\n",
       " {'Bad Luck Brian'},\n",
       " {'Scumbag Genetics'},\n",
       " {'Scumbag Redditor'},\n",
       " {'Annoyed Picard'},\n",
       " {'Scumbag Teacher'},\n",
       " {'Scumbag Teacher'},\n",
       " {'Annoyed Picard'},\n",
       " {'Push it somewhere else Patrick'},\n",
       " {'1990s Problems'},\n",
       " {'1990s Problems'},\n",
       " {'skeptical baby'},\n",
       " {'Ron Paul'},\n",
       " {'Scumbag Redditor'},\n",
       " {'Pickup-Line Panda', 'Ridiculously photogenic guy'},\n",
       " {'Bad Joke Eel'},\n",
       " {'Scumbag Brain'},\n",
       " {'Scumbag Redditor'},\n",
       " {'1990s Problems', 'Bad Luck Brian'},\n",
       " {'Annoyed Picard'},\n",
       " {'skeptical baby'},\n",
       " {'Ron Paul'},\n",
       " {'Almost Politically Correct Redneck', 'Annoyed Picard'},\n",
       " {'1990s Problems'},\n",
       " {'Almost Politically Correct Redneck', 'Annoyed Picard'},\n",
       " {'Ron Paul'},\n",
       " {'Annoyed Picard', 'Bad Joke Eel'},\n",
       " {'Bad Luck Brian'},\n",
       " {'Bad Luck Brian'},\n",
       " {'Socially Awkward Awesome Penguin'},\n",
       " {'skeptical baby'},\n",
       " {'Captain Hindsight', 'Scumbag Redditor'},\n",
       " {'Ron Paul'},\n",
       " {'Scumbag Genetics'},\n",
       " {'Ron Paul'},\n",
       " {'Scumbag Brain'},\n",
       " {'Scumbag Redditor'},\n",
       " {'Almost Politically Correct Redneck', 'Bad Joke Eel'},\n",
       " {'skeptical baby'},\n",
       " {'Ridiculously photogenic guy'},\n",
       " {'Bad Luck Brian', 'Scumbag Genetics'},\n",
       " {'Bad Joke Eel'},\n",
       " {'Push it somewhere else Patrick'},\n",
       " {'Scumbag Redditor'},\n",
       " {'Scumbag Teacher'},\n",
       " {'Bad Luck Brian', 'Scumbag Brain', 'Scumbag Genetics'},\n",
       " {'Annoyed Picard', 'Bad Luck Brian'},\n",
       " {'Scumbag Genetics'},\n",
       " {'Ron Paul'},\n",
       " {'Schrute', 'Scumbag Redditor'},\n",
       " {'One Does Not Simply'},\n",
       " {'Inception', 'Socially Awkward Awesome Penguin'},\n",
       " {'Scumbag Genetics'},\n",
       " {'1990s Problems', 'Bad Luck Brian', 'Scumbag Genetics'},\n",
       " {'One Does Not Simply'},\n",
       " {'Pickup-Line Panda'},\n",
       " {'The Most Interesting Man In The World'},\n",
       " {'skeptical baby'}]"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is are the set versions of the meme gorups\n",
    "[{meme_temps[meme] for meme in group.memes} for group in mc.meme_groups]\n",
    "\n",
    "# If you want to see the actual group distributions, then uncomment and run this. Its 20,000 entries.\n",
    "# sorted([[meme_temps[meme] for meme in group.memes] for group in mc.meme_groups], key = lambda x: -len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_scores = dict()\n",
    "\n",
    "for meme_group in [x for x in mc.meme_groups if len(x.memes) > 5]:\n",
    "    templates = [meme_temps[x] for x in meme_group.memes]\n",
    "    group_mode = max(templates, key = templates.count)\n",
    "    score = templates.count(group_mode) / len(templates)\n",
    "    \n",
    "    classification_scores[meme_group.group_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9617572519411453"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the average \"purity\" of each meme group\n",
    "np.mean(list(classification_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model works fairly well at distinguishing memes. It outperformed other models I attemped such as gradient boosting trees and logistic regression. In terms of speed, it is only slightly slower than logistic regression, the fastest model. It can do so with ~98% precision and ~98% recall on randomly selected pairs of memes in the dataframe. \n",
    "\n",
    "The model's performance can probably be improved. In fact, the model returned \"false positives\" on memes that were allegedly different such as \"Annoyed Picard HD\" and \"Annoyed Picard\". Obviously, the memes are actually the same and our model was actually correct. This revealed to me that many of the memes in the dataset were mislabeled. This made the project even harder to move forward with as I spent a good amount of time veryfying the labels in the dataframe and suspect that there are still mistakes in it.\n",
    "\n",
    "However, I didn't have as much time to improve the clustering method (MemeClassifier). I tried to use other methods to cluster the memes such as DBSCAN but speed was a large issue in both testing methods and running them in full. For instance, a full classification of all 170,000+ memes in the dataframe took about 17 hours to complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
